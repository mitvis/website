---
title: 'Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs'
authors:
  - key: hsuresh
  - name: Steven R. Gomez
    affiliation: MIT Lincoln Laboratory
  - name: Kevin K. Nam
    affiliation: MIT Lincoln Laboratory
  - key: arvindsatya
venue: chi
year: 2021
doi: 10.1145/3411764.3445088
tags:
  - machine learning interpretability
  - qualitative methods
teaser: A state of the <a href="/pubs/beyond-expertise-roles/framework-connections/">interactive figure</a> that visualizes the results of the analysis of ourframeworkâ€™s descriptive power. We see how the two halves of the framework (knowledge-contexts and goals-objectives-tasks) provide a more granular and composable vocabulary with which to describe 58 papers from the literature on ML interpretability. Light grey links represent the set of all papers, and connect codes that appear together. The width of the link correspondsto the number of papers it represents. We use "code undetermined" to indicate cases where we were not able to code a particular category (e.g., if a paper did not explicitly specify a knowledge-context). In the interactive figure, hovering over a code selects all papers that contain the code, and highlights links to visualize the co-occurrence of other codes (e.g., "O2" shown here).
videos:
  - name: CHI 2021 Talk
    key: CGbKmlTzRLI
materials:
  - name: Interactive Figure of Framework Connections
    url: /pubs/beyond-expertise-roles/framework-connections/
    type: file
---
To ensure accountability and mitigate harm, it is critical that diverse stakeholders can interrogate black-box automated systems and find information that is understandable, relevant, and useful to them. In this paper, we eschew prior expertise- and role-based categorizations of interpretability stakeholders in favor of a more granular framework that decouples stakeholders' knowledge from their interpretability needs. We characterize stakeholders by their formal, instrumental, and personal knowledge and how it manifests in the contexts of machine learning, the data domain, and the general milieu. We additionally distill a hierarchical typology of stakeholder needs that distinguishes higher-level domain goals from lower-level interpretability tasks. In assessing the descriptive, evaluative, and generative powers of our framework, we find our more nuanced treatment of stakeholders reveals gaps and opportunities in the interpretability literature, adds precision to the design and comparison of user studies, and facilitates a more reflexive approach to conducting this research.
