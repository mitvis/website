---
title: 'Kaleidoscope: Semantically-grounded, Context-specific ML Model Evaluation'
authors:
  - key: hsuresh
  - name: Divya Shanmugam
    affiliation: MIT CSAIL
  - key: tiffc
  - key: annieb22
  - name: Alexander D'Amour
    affiliation: Google Research
  - name: John V. Guttag
    affiliation: MIT CSAIL
  - key: arvindsatya
venue: chi
year: 2023
doi: 10.1145/3544548.3581482
tags:
  - machine learning interpretability
  - interface
  - qualitative methods
videos:
  - name: Demo Video
    key: NKk8yBq9wpo
  - name: CHI 2023 Talk
    key: Vu0cp_81Rkc
materials:
  - name: Code
    url: /pubs/kaleidoscope.zip
    type: file
teaser: 'Kaleidoscopeâ€™s workflow consists of identifying meaningful examples, generalizing them into larger, diverse sets representing important concepts, and using these concepts to specify and test model behavior.'
---
To ensure accountability and mitigate harm, it is critical that diverse stakeholders can interrogate black-box automated systems and find information that is understandable, relevant, and useful to them. In this paper, we eschew prior expertise- and role-based categorizations of interpretability stakeholders in favor of a more granular framework that decouples stakeholders' knowledge from their interpretability needs. We characterize stakeholders by their formal, instrumental, and personal knowledge and how it manifests in the contexts of machine learning, the data domain, and the general milieu. We additionally distill a hierarchical typology of stakeholder needs that distinguishes higher-level domain goals from lower-level interpretability tasks. In assessing the descriptive, evaluative, and generative powers of our framework, we find our more nuanced treatment of stakeholders reveals gaps and opportunities in the interpretability literature, adds precision to the design and comparison of user studies, and facilitates a more reflexive approach to conducting this research.
