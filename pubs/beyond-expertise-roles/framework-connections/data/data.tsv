bibtex	year	author	title	url	venue	ML-Formal	ML-Instrumental	ML-Personal	Domain-Formal	Domain-Instrumental	Domain-Personal	Milieu-Formal	Milieu-Instrumental	Milieu-Personal	codes
cai_hello_2019	2019	"Cai, Carrie J.; Winter, Samantha; Steiner, David; Wilcox, Lauren; Terry, Michael"	Hello AI: Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making	https://dl.acm.org/doi/10.1145/3359206	Proceedings of the ACM on Human-Computer Interaction		"medical experts’ increasing familiarity with [computer-aided diagnosis] systems; ""user expectations of an AI’s performance and metrics can be strongly anchored to their prior experience with detection-based models (section 5.1.1); as seen in our study, this can sometimes yield unrealistic expectations."""		pathologists	clinicians			"“More variation is better...Covering from community hospital small groups, to academic medical centers, it’s more representative.” (P16)"		"G1: ""global transparency questions could be key to forming an accurate initial impression of an ML-based system""; O4: ""struggled to determine the extent to which its diagnostic process could be similar to or different from their own""; O7: “It’s hard for me to know how much to trust when I see things that I don’t completely agree with.”"
lakkaraju_how_2020	2020	"Lakkaraju, Himabindu; Bastani, Osbert"	How do I fool you?: Manipulating User Trust via Misleading Black Box Explanations	https://doi.org/10.1145/3375627.3375833	"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society"				"Each participant is a student enrolled in a law school,  ""Each participant acknowledged having in-depth knowledge (16 participants) or at least some familiarity (31 participants) with the bail decision making process. """						G2
mohseni_human-grounded_2020	2020	"Mohseni, Sina; Block, Jeremy E.; Ragan, Eric D."	A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning	http://arxiv.org/abs/1801.05075	arXiv:1801.05075 [cs]										"G1: ""the ultimate goal is for people to understand machine models"""
mohseni_multidisciplinary_2020	2020	"Mohseni, Sina; Zarei, Niloofar; Ragan, Eric D."	A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems	http://arxiv.org/abs/1811.11839	arXiv:1811.11839 [cs]	AI experts are machine learning scientists and engineers who design machine learning algorithms	"Data experts include data scientists and domain experts who use machine learning for analysis, decision-making, or research"						AI novices refer to end-users who use AI products in daily life		"G2: ""User Trust and Reliance""; O5: ""Machine learning explanations can disclose to end-users what user data is being used in algorithmic decision-making"", bias mitigation, privacy awareness; O1: ""help data experts to tune machine learning parameters for their specific data"", model debugging"
yu_user-based_2018	2018	"Yu, Rulei; Shi, Lei"	A user-based taxonomy for deep learning visualization	http://www.sciencedirect.com/science/article/pii/S2468502X1830038X	Visual Informatics	"Experts: This group of users are experienced in deep learning, and they wish to have a brief idea of the research field in deep learning visualization."	"Practitioners: Users who already have some experience and understand what deep learning is. They want to get the intuition of how the architectures look like when dealing with deep learning models.  ""Developers: Users who design and improve deep learning models, find it difficult to debug during training process and tune the parameters."""	It is essential for experts and developers to quickly build the intuition of how the network looks like							"O6: learning about ML, ""They want to get the intuition of how the architectures look like when dealing with deep learning models""; O4: ""While it is difficult for users to comprehend the results of representation learning, analysis of the complicated and superimposed deep neural networks can be very challenging""; O1: ""optimizing the model of specific tasks, accelerating the training, or fine-tuning the parameters"""
boukhelifa_exploratory_2019	2019	"Boukhelifa, Nadia; Bezerianos, Anastasia; Trelea, Ioan Cristian; Perrot, Nathalie Méjean; Lutton, Evelyne"	An Exploratory Study on Visual Exploration of Model Simulations by Multiple Types of Experts	http://dl.acm.org/citation.cfm?doid=3290605.3300874	Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI '19	model builders	optimization expertise		agronomic engineers	only specialists in part of the underlying process					"O4: Explaining findings through storytelling to collaborators; T1: ""Identify and explain an outlier"""
bansal_beyond_2019	2019	"Bansal, Gagan; Nushi, Besmira; Kamar, Ece; Lasecki, Walter; Weld, Daniel S; Horvitz, Eric"	Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance	https://www.aaai.org/ojs/index.php/HCOMP/article/download/5285/5137	HCOMP 2019				"AI inferences to help human experts make better decisions, e.g., with respect to medical diagnoses, recidivism prediction, and credit assessment"						G1; T5: understanding model error from predictions
raji2020closing	2020	"Raji, Inioluwa Deborah; Smart, Andrew; White, Rebecca N.; Mitchell, Margaret; Gebru, Timnit; Hutchinson, Ben; Smith-Loud, Jamila; Theron, Daniel; Barnes, Parker"	Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing	http://dl.acm.org/doi/10.1145/3351095.3372873	"Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency"					Internal financial auditors				"what type of model behavior might be experienced by different cultural, demographic or phenotypic groups."	"O2: ""frameworks through which independent audits can demonstrate adherence to standards""; T2: ""it is incumbent on producers of artificial intelligence systems to anticipate ethics-related failures before launch""; T3: ""data entanglement"""
byrne_counterfactuals_2019	2019	"Byrne, Ruth M. J."	Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning	https://www.ijcai.org/proceedings/2019/876	Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence										"G2: ""To increase trust in such systems by human users""; O4: ""It enables the user to consider contrastive explanations and counterfactual analyses, such as why one decision was made instead of another"""
chromik_dark_2019	2019	"Chromik, Michael; Eiband, Malin; Völkel, Sarah Theres; Buschek, Daniel"	"Dark Patterns of Explainability, Transparency, and User Control for Intelligent Systems"	http://ceur-ws.org/Vol-2327/IUI19WS-ExSS2019-7.pdf	IUI Workshops 2019						operators might be more interested how well the system’s conceptual model fits their mental model 				"T4: ""interested in the factors influencing their individual decision""; O1; O5; O7: ""present an incontestable subset of reasons to the bank employee"""
bansal_does_2020	2020	"Bansal, Gagan; Wu, Tongshuang; Zhou, Joyce; Fok, Raymond; Nushi, Besmira; Kamar, Ece; Ribeiro, Marco Tulio; Weld, Daniel S."	Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance	http://arxiv.org/abs/2006.14779	arXiv:2006.14779 [cs]				"doctors, judges"					"Turkers doing sentiment analysis, ""followed my own discretion"""	"G2: build appropriate trust; O4; T1: ""may have learned to rely on [the explanations] less""; T5: ""know when to trust the AI’s suggestion and when to be skeptical"""
wolf_explainability_2019	2019	"Wolf, Christine T."	Explainability scenarios: towards scenario-based XAI design	https://dl.acm.org/doi/10.1145/3301275.3302317	Proceedings of the 24th International Conference on Intelligent User Interfaces									"we see how a range of actors must make sense of an AI output – yet each of these actors bring their own points-of-view and own priorities, which can sometimes be in conflict. "	"O7: contest AI recommendation about assisted living; T2: bias detection, ""consider the experimental system’s predictions with their established ways of monitoring ADLs"""
roscher_explainable_2020	2020	"Roscher, Ribana; Bohn, Bastian; Duarte, Marco F.; Garcke, Jochen"	Explainable Machine Learning for Scientific Insights and Discoveries	https://arxiv.org/abs/1905.08883	IEEE Access	Model developers			scientists	scientists increasingly adopt ML for optimizing and producing scientific outcomes					"G1: ""to attain scientific outcomes with ML one wants an understanding""; T1: ""to ensure the scientific value of the outcome""; T3: ""the user can be informed when the scheme is being used for systems for which it is not suited""; O1; O2; O4"
bhatt_explainable_2020	2020	"Bhatt, Umang; Xiang, Alice; Sharma, Shubham; Weller, Adrian; Taly, Ankur; Jia, Yunhan; Ghosh, Joydeep; Puri, Ruchir; Moura, José M. F.; Eckersley, Peter"	Explainable machine learning in deployment	https://arxiv.org/abs/1909.06342	"Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency"	 ML Engineers	data scientists		regulators						"O2; O1; T4: ""how drift in feature distributions would impact model outcomes"""
spinner_explainer_2020	2020	"Spinner, Thilo; Schlegel, Udo; Schäfer, Hanna; El-Assady, Mennatallah"	explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning	http://arxiv.org/abs/1908.00087	IEEE Transactions on Visualization and Computer Graphics	model developers	model users		biologist (model user)	"model novices (These are non-experts in ML, interested in understanding ML concepts and getting to know more about applying ML models, e.g., for specific domains.)"					"O4: ""enable a reasoned justification of the user’s decision-making""; O1: model refinement; O2; G2: trust-building steering mechanism"
gilpin_explaining_2018	2018	"Gilpin, Leilani H.; Bau, David; Yuan, Ben Z.; Bajwa, Ayesha; Specter, Michael; Kagal, Lalana"	Explaining Explanations: An Overview of Interpretability of Machine Learning	https://ieeexplore.ieee.org/abstract/document/8631448?casa_token=3Ni1cJQ_UBEAAAAA:uMptp60MAR29lYviC8ZBiCWki1OI4NmC5HRAZSuSNgDhzHmJJVwNq6V1nlWM2eVNqncnVrss	2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)		broadly engage the greater machine learning community								"G1; G2; O2: ""With impending regulations like the European Union’s 'Right to Explanation'"""
miller_explanation_2019	2019	"Miller, Tim"	Explanation in artificial intelligence: Insights from the social sciences	https://linkinghub.elsevier.com/retrieve/pii/S0004370218305988	Artificial Intelligence	The very experts who understand decision-making models		the researchers' intuition of what constitutes a ‘good’ explanation.			they will construct a better mental model of the system and be able to generalise its behaviour (effectively learning its model)			"people employ certain biases and social expectations when they generate and evaluate explanation, ""the social behaviour of others in physical environments"""	"G2: ""to understand and therefore trust the intelligent agents""; O4: ""For explanation, if the goal of an explanatory agent is to provide the most likely causes of an event, then these three criteria can be used to prioritise among the most likely events. However, if the goal of an explanatory agent is to generate trust between itself and its human observers, these criteria should be considered as first-class criteria in explanation generation beside or even above likelihood."""
ras_explanation_2018	2018	"Ras, Gabriëlle; van Gerven, Marcel; Haselager, Pim"	"Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges"	https://doi.org/10.1007/978-3-319-98131-4_2	Explainable and Interpretable Models in Computer Vision and Machine Learning	DNN engineers 	DNN developers					ethicists	Stakeholders are often interested in the ethical and legal concerns raised in any phase of the process.	"Owner, end user, data subject"	"O1; O2; T4: ""did the factor 'race' influence the outcome of the system”"
tullio_how_2007	2007	"Tullio, Joe; Dey, Anind K.; Chalecki, Jason; Fogarty, James"	How it works: a field study of non-technical users interacting with an intelligent system	https://doi.org/10.1145/1240624.1240630	Proceedings of the SIGCHI Conference on Human Factors in Computing Systems				HR managers who produce expert estimates	Direct reports who use the estimates				Subject already determines interruptibility which the paper is trying to enhance using an intelligent system	"O3; T3: understand ""what the system was sensing in order to make its inferences"""
hong_human_2020	2020	"Hong, Sungsoo Ray; Hullman, Jessica; Bertini, Enrico"	"Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs"	https://dl.acm.org/doi/10.1145/3392878	Proceedings of the ACM on Human-Computer Interaction		"industry practitioners, ""data scientists or machine learning engineers, 2 identified themselves as software engineers whose goal is to build infrastructure related to model interpretability"""			Various domain users listed in Table 1.			product managers		"G2: ""often aimed at building trust ""; T5: ""need for model builders to gain confidence in the reliability and validity of models they build""; T4: ""understand the mechanism by which a model makes predictions""; O4: ""explanations or justifications for model decisions being most often requested""; O1: ""identify issues with a model and devise ways to fix it""; O6: ""as a vehicle to generate insights about the phenomena described by the data"""
tomsett_interpretable_2018	2018	"Tomsett, Richard; Braines, Dave; Harborne, Dan; Preece, Alun; Chakraborty, Supriyo"	Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems	http://arxiv.org/abs/1806.07552	arXiv:1806.07552 [cs]	Creators	Implementers		"business logic for this decision will have been decided on by more senior employees at the lender; ""Examiners"" - ""tasked with compliance/safety-testing, auditing, or forensically investigating a system"""	Operators	"decision-subjects, ""data-subjects"""		Examiners		"G2; O4: ""recommends possible treatment options that the doctor can then discuss with the patient""; O4: "" After the event, this [military ML] order may be scrutinized at a tribunal.""; O7: ""idea of contestability""; O5: ""Data-subjects may have moral concerns about how their data is being used to make decisions about other people""; O2: forensics, ""aid in their auditing or forensic investigations of ecosystems. They may focus on decision-subjects and data-subjects, ensuring that the system is compliant with their rights"""
kaur_interpreting_2020	2020	"Kaur, Harmanpreet; Nori, Harsha; Jenkins, Samuel; Caruana, Rich; Wallach, Hanna; Wortman Vaughan, Jennifer"	Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning	https://dl.acm.org/doi/10.1145/3313831.3376219	Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems		"Data scientists and Machine Learning practitioners ""study data scientists’ use of two existing interpretability tools"""				"Without accurate mental models, social factors can rationalize suspicious observations [about explanations]"				"O4: ""[using interpretability visualizations to] uncover [...] missing values in a dataset that have been filled in incorrectly"", reasoning about data outputs"
vayena_machine_2018	2018	"Vayena, Effy; Blasimme, Alessandro; Cohen, I. Glenn"	Machine learning in medicine: Addressing ethical challenges	https://dx.plos.org/10.1371/journal.pmed.1002689	PLOS Medicine				end users—doctors		shared decision-making [...] meaningful dialogue between healthcare providers and patients (presumably for the patient to share lived experience)	"bodies such as institutional review boards, ethics review committees, and health technology assessment organizations"			"O2: ""Some degree of explainability may also be required to justify the clinical validation of MLm in prospective studies and randomized clinical trial""; G2: ""lack of transparency can preclude the mechanistic interpretation of MLm-based assessments and, in turn, reduce their trustworthiness"""
balog_measuring_2020	2020	"Balog, Krisztian; Radlinski, Filip"	Measuring Recommendation Explanation Quality:The Conflicting Goals of Explanations	https://storage.googleapis.com/pub-tools-public-publication-data/pdf/4634449570caa1416f4729827e371308ff6d26f2.pdf	Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20)										"G2: trust, increase user confidence in system; O3"
arya_one_2019	2019	"Arya, Vijay; Bellamy, Rachel K. E.; Chen, Pin-Yu; Dhurandhar, Amit; Hind, Michael; Hoffman, Samuel C.; Houde, Stephanie; Liao, Q. Vera; Luss, Ronny; Mojsilović, Aleksandra; Mourad, Sami; Pedemonte, Pablo; Raghavendra, Ramya; Richards, John; Sattigeri, Prasanna; Shanmugam, Karthikeyan; Singh, Moninder; Varshney, Kush R.; Wei, Dennis; Zhang, Yunfeng"	One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques	http://arxiv.org/abs/1909.03012	"arXiv:1909.03012 [cs, stat]"		"data scientist users, who may not be experts in explainability, as well as algorithm developers"			specialist with deep knowledge of the circumstances for employee retention				loan applicant	"O4: ""validating whether recommendations given by the model for different loan applications are justified""; O5; O3: ""know about a few factors that could be changed to improve their profile for possible approval in the future"""
das_opportunities_2020	2020	"Das, Arun; Rad, Paul"	Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey	http://arxiv.org/abs/2006.11371	arXiv:2006.11371 [cs]										"T2: ""Learning the model behavior using XAI techniques for different input data distributions could improve our understanding of the skewness and biases in the input data""; G1; G2: ""Improves trust"", definition of ""trustability""; O4"
adadi_peeking_2018	2018	"Adadi, Amina; Berrada, Mohammed"	Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)	https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8466590	IEEE Access										"O4: ""provides the required information to justify result""; O1: ""A model that can be explained and understood is one that can be more easily improved.""; O2: ""compliance with legislation, for instance the 'right to explanation'""; O6: ""Asking for explanations is a helpful tool to learn new facts, to gather information and thus to gain knowledge""; T2: ""provides greater visibility over unknown vulnerabilities and flaws"""
schneider_personalized_2019	2019	"Schneider, Johannes; Handali, Joshua"	Personalized Explanation for Machine Learning: A Conceptualization	https://aisel.aisnet.org/ecis2019_rp/171	Proceedings of the 27th European Conference on Information Systems (ECIS)		ML engineer			doctor [using a disease recognition system]	patient using a disease recognition system 			Decision Information [...] general knowledge or experiences	"O6: ""Users were supposed to learn to solve the task, ie. image classification based on the explanations provided"""
bucinca_proxy_2020	2020	"Buçinca, Zana; Lin, Phoebe; Gajos, Krzysztof Z.; Glassman, Elena L."	Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems	http://arxiv.org/abs/2001.08298	Proceedings of the 25th International Conference on Intelligent User Interfaces		"primarily graduate students with backgrounds from design, biomedical engineering, and education. Participants had varying levels of experience with AI and machine learning, ranging from 0–5 years of experience."								"G2: ""the goal of explainable interfaces should be instilling in users the right amount of trust""; O4; O3: ""were asked their own [domain-related] decision [using the XAI or not]"""
liao_questioning_2020	2020	"Liao, Q. Vera; Gruen, Daniel; Miller, Sarah"	Questioning the AI: Informing Design Practices for Explainable AI User Experiences	https://doi.org/10.1145/3313831.3376590	Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems		industry practitioners to create explainable AI products				"lay users, who may not have deep technical understanding of AI, but hold preconception of what constitutes useful explanations for decisions"		"UX and design practitioners, ""regulators"""		"T4: ""inspect how the output changes with instance changes""; G2: ""Explanation is often embraced as a cure for 'black box' models to gain user trust and adoption""; T1: ""to assess the AI’s judgment to make an informed decision""; T5: ""beware of the system’s limitations""; O1: ""improve AI performance""; O3: “how the output impacts other system components"""
lipton_mythos_2018	2018	"Lipton, Zachary C."	The mythos of model interpretability	https://dl.acm.org/doi/10.1145/3233231	Communications of the ACM										"G2; O6: ""uncover causal structure in observational data""; O5; O2; O7: ""contest this premise in the hope of overturning the decision""; T4: ""by probing the patterns that the model has extracted, we can convey additional information to a human decision maker"""
glass_toward_2008	2008	"Glass, Alyssa; McGuinness, Deborah L.; Wolverton, Michael"	Toward establishing trust in adaptive agents	http://portal.acm.org/citation.cfm?doid=1378773.1378804	Proceedings of the 13th international conference on Intelligent user interfaces - IUI '08		Several had helped to develop individual components of the system. Others were managers or researchers contributing to individual aspects of the system								"G1: ""need to understand the agent’s behavior and responses enough to participate in the mixed-initiative execution process""; G2: ""need to trust the reasoning and actions performed by the agent""; T3: ""knowing what resources were being used to provide answers"""
schlegel_towards_2019	2019	"Schlegel, Udo; Arnout, Hiba; El-Assady, Mennatallah; Oelke, Daniela; Keim, Daniel A."	Towards a Rigorous Evaluation of XAI Methods on Time Series	http://arxiv.org/abs/1909.07082	arXiv:1909.07082 [cs]					"with domain knowledge, an expert can inspect the produced explanation visualizations to verify the result qualitatively"					"O1: ""To debug and optimize time series prediction models in diverse tasks, not only understanding is essential but also that the XAI explanation is correct itself"""
doshi-velez_towards_2017	2017	"Doshi-Velez, Finale; Kim, Been"	Towards A Rigorous Science of Interpretable Machine Learning	http://arxiv.org/abs/1702.08608	"arXiv:1702.08608 [cs, stat]"	machine learning researchers			doctors performing diagnoses	clinician					"G1; O2: GDPR will ""require algorithms that make decisions based on user-level predictors""; O6: ""The human’s goal is to gain knowledge""; T2: ""there might be biases that we did not consider a priori""; O3: ""such as identifying errors in a safety-oriented task"""
weller_transparency_2019	2019	"Weller, Adrian"	Transparency: Motivations and Challenges	http://arxiv.org/abs/1708.01870	arXiv:1708.01870 [cs]		An engineering perspective might naturally lead one to suspect that more information should lead to a better outcome		the background of an economist or multi-agent game theorist helps to realize that more information empowers the agents to optimize their own agendas more efficiently		"patient, ""client"""				"O4: ""the ability to audit a prediction or decision trail in detail, particularly if something goes wrong""; G1; O7; O2: ""To facilitate monitoring and testing for safety standards""; O1: ""For a developer, to understand how their system is working, aiming to debug or improve it"""
abdul_trends_2018	2018	"Abdul, Ashraf; Vermeulen, Jo; Wang, Danding; Lim, Brian Y.; Kankanhalli, Mohan"	"Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda"	http://dl.acm.org/citation.cfm?doid=3173574.3174156	Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI '18		data scientists					require understanding requirements arising from social contexts other than just from usability or human cognitive psychology			"G2; G1; O4: ""people needed to be able to understand what was being sensed and which actions were being taken based on that information""; T4: ""how feedforward can help people understand and predict what is going to happen"""
yin_understanding_2019	2019	"Yin, Ming; Wortman Vaughan, Jennifer; Wallach, Hanna"	Understanding the Effect of Accuracy on Trust in Machine Learning Models	https://doi.org/10.1145/3290605.3300509	Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems					end users of systems built using ML models	trust in the model is significantly affected by its observed accuracy regardless of its stated accuracy				"G1; G2; T1: ""after observing a model’s accuracy in practice, people are more likely to increase their trust in the model if the model’s observed accuracy is higher than their own accuracy"""
hohman_visual_2019	2019	"Hohman, Fred; Kahng, Minsuk; Pienta, Robert; Chau, Duen Horng"	Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers	https://arxiv.org/pdf/1801.06889.pdf	IEEE Transactions on Visualization and Computer Graphics	Model-developers and builders	"These are users who may have some technical background but are neural network novices   "". Common tasks include using well-known neural network architectures for developing domain specific applications, training smaller-scale models, and downloading pre-trained model weights online to use as a starting point. This group of users also include machine learning artists who use models to enable and showcase new forms of artistic expression."	well-developed intuition...mastery over models... vary hyperparameters					"These are individuals who typically have no prior knowledge about deep learning, and may or may not have a technical background. Much of the research targeted at this group is for educational purposes, trying to explain what a neural network is and how it works at a high-level, sometimes without revealing deep learning is present. These group also includes people who simply use AI-powered devices and consumer applications.  ""These group also includes people who simply use AI-powered devices and consumer applications."""		O4: explain decisions made by medical imaging models; O1: debugging
ferreira2020people	2020	"Ferreira, Juliana J.; Monteiro, Mateus S."	What Are People Doing About XAI User Experience? A Survey on AI Explainability Research and Practice	https://link.springer.com/chapter/10.1007/978-3-030-49760-6_4	"Design, User Experience, and Usability. Design for Contemporary Interactive Environments"	"ML experts, which are people capable of building, training and testing machine learning models with different datasets from different domains."	Non-ML-experts ...[who] use ML tools to perform tasks..							"non-experts in context-agnostic contexts, discussing privacy and personal data"	"O1: ""using explanations to improve an aspect, a part of the system, among others""; G2; O2: GDPR"
tonekaboni_what_2019	2019	"Tonekaboni, Sana; Joshi, Shalmali; McCradden, Melissa D.; Goldenberg, Anna"	What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use	http://proceedings.mlr.press/v106/tonekaboni19a.html	Machine Learning for Healthcare Conference		"Clinicians repeatedly identified that knowing the subset of features deriving the model outcome, is crucial"		"clinicians from two distinct acute care specialties (Intenstive Care Unit and Emergency Department), Provide a level of transparency that allows users to validate model outputs with domain knowledge."""	clinicians	critical factor for continued usage was whether the tool was repeatedly successful in prognosticating their patient’s condition in their personal experience.		"[Interview] settings were chosen because both departments are areas where we foresee implementation of ML tools to support clinical care. Additionally, both departments have current experience working with either early warning/alert systems or opaque, non-ML decision support tools"	"Many clinicians noted that “alarm or click fatigue” (indicating the annoyance with repeating response prompts through the EHR system) (Embi and Leonard, 2012) is a significant concern that may be worsened by prediction tools. This issue is ubiqitous across healthcare contexts and requires careful consideration"	"O4: ""viewed explainability as a means of justifying their clinical decision-making (for instance, to patients and colleagues)""; G2: ""Building trust between clinicians and ML models""; O3: ""facilitate trust in the model as well as directing use in specific patient populations and determining parameters guiding appropriate use""; T5: ""clarity around why the model under-performs"""
brennen_what_2020	2020	"Brennen, Andrea"	"What Do People Really Want When They Say They Want ""Explainable AI?"" We Asked 60 Stakeholders."	https://doi.org/10.1145/3334480.3383047	Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems		" ""data scientist""; ""end users of ML and AI systems"""				a lay person affected by an automated decision likely has his or her own intuitive notion of what outcomes seem fair or just. It is easy to imagine an affected individual dismissing an outcome		"When we shared the findings from this technology study with others in government and industry, we saw that today’s XAI tools do not fully capture the types of explanations that many people want."		"G2; G1: ""they wanted better tools to determine if their code was executing properly, to confirm that code was doing what they thought it was doing, and to help them build intuition about how models worked.""; O2: ""Explainable AI as an important means of auditing models"""
ribera2019can	2019	Riberiera & Lapedriza	Can we do better explanations? A proposal of User-Centered Explainable AI	http://ceur-ws.org/Vol-2327/IUI19WS-ExSS2019-12.pdf	Joint Proceedings of the ACM IUI 2019 Workshops	"Developers and AI researchers: investigators in AI, software developers, or data analysts who create the AI system.  KN: * Lay users: ""with no technical background, would not be interested at all about these type of explanation"""			Domain experts: specialists in the area of expertise where the decisions made by the system belong to. For example: physicists or lawyers.		"Lay users: the final recipients of the decisions. For example: a person accepted or rejected on a loan demand, or a patient that has been diagnosed  KN: Not sure if they mention any knowledge status of lay users, although it would be strange to include AI researchers and domain experts, and not lay users just because no explanation was provided for the lay users.  HS: yeah, I generally put users described as ""decision subjects"" under milieu-personal in the few I coded.  "				O1: improvement; T2: verification to detect bias; O6: learn from the system; O3: right to an explanation that has consequences; O2: compliance with legislation
cheng2019explaining	2019	Cheng et al	Explaining Decision-Making Algorithms through UI: Strategies to Help Non-Expert Stakeholders	https://dl-acm-org.libproxy.mit.edu/doi/abs/10.1145/3290605.3300789	Proceedings of CHI Conference on Human Factors in Computing Systems (CHI '19)					"According to Model 6, higher technical literacy was associated with higher levels of self-reported understanding (Coef. = 0.59, p<0.001)."					G2; G1: help users and other stakeholders understand the “algorithmic decision model”; T3: see what attributes cause the algorithm’s action; T4: the user can predict how changes in the situation can lead to alternative algorithm predictions
kizilcec2016much	2016	Kizilcec	How Much Information? Effects of Transparency on Trust in an Algorithmic Interface	https://dl-acm-org.libproxy.mit.edu/doi/pdf/10.1145/2858036.2858402						participants of a MOOC course doing peer grading					"G2: Trust is a key concern in the design of technology, as it affects the initial adoption and continued use of technologies"
paudyal2019learn2sign	2019	"Paudyal, P., Lee, J., Kamzin, A., Soudki, M., Banerjee, A., Gupta, S.K."	Learn2sign: explainable AI for sign language learning	https://explainablesystems.comp.nus.edu.sg/2019/wp-content/uploads/2019/02/IUI19WS-ExSS2019-13.pdf	Joint Proceedings of the ACM IUI 2019 Workshop					learners of sign language					O6: learn about sign language and how to do it correctly; O3: understand how to correct actions based on model feedback
sundararajan2019exploring	2019	"Sundararajan, M., Xu, J., Taly, A., Sayres, R., Najmi, A."	Exploring Principled Visualizations for Deep Network Attributions	http://ceur-ws.org/Vol-2327/IUI19WS-ExSS2019-16.pdf		model developers			doctors						"O1: guide model or training data improvements; O3: assist a human decision-maker, such as a doctor"
nunes2017systematic	2017	Nunnes & Jannach	A Systematic Review and Taxonomy of Explanations in Decision Support and Recommender Systems	https://www.researchgate.net/publication/342229818_A_systematic_review_and_taxonomy_of_explanations_in_decision_support_and_recommender_systems											"G1: explain how the system works; O3: help make decisions better and faster, persuade users to try or buy; O1: debugging; G2; T2: detect mistakes"
xie2019outlining	2019	"Xie, Y., Gao, G., Chen, X.:"	Outlining the Design Space of Explainable Intelligent Systems for Medical Diagnosis	https://arxiv.org/abs/1902.06019					doctors						"O3: ""diagnosis and treatment purposes in their daily work practice"""
zhaO3019transparency	2019	"Zhao, R., Benbasat, I., Cavusoglu, H.:"	Transparency in Advice-Giving Systems: A Framework and a Research Model for Transparency Provision	http://ceur-ws.org/Vol-2327/IUI19WS-IUIATEC-2.pdf										recommendation system users	"O4: ""what they do (e.g. how AGSs generate advice)""; O5: why certain data is collected; T3: ""What/how data is collected"""
krebs2019tell	2019	"Krebs, L.M., et al"	Tell Me What You Know: GDPR Implications on Designing Transparency and Accountability for News Recommender Systems1	https://www.semanticscholar.org/paper/Tell-Me-What-You-Know%3A-GDPR-Implications-on-and-for-Krebs-Rodriguez/81f030002cc3eb57e74570a4e20b01a26ebb6ef1										news recommender users	O2: compliance w/ GDPR
cai2019effects	2019	"Cai, C.J., Jongejan, J., Holbrook, J."	The effects of example-based explanations in a machine learning interface. In: Proceedings of the 24th International Conference on Intelligent User Interfaces	https://dl-acm-org.libproxy.mit.edu/doi/10.1145/3301275.3302289											"O4: give users insight into why the system recognized or did not recognize their drawing; T4: understanding what about the drawing changes the prediction; G1; G2: ""the questions assessed system understanding [...] as well as capability and benevolence, key dimensions of trust"""
bhatia2019explainable	2019	"Bhatia, A., Garg, V., Haves, P., Pudi, V.:"	Explainable clustering using hyper-rectangles for building energy simulation data	https://www.researchgate.net/publication/331500351_Explainable_Clustering_Using_Hyper-Rectangles_for_Building_Energy_Simulation_Data					energy data operators						O6: learn design strategies based on cluster commonalities 
theodorou2017designing	2017	"A Theodorou, RH Wortham, JJ Bryson"	Designing and implementing transparency for real time inspection of autonomous robots	https://www-tandfonline-com.libproxy.mit.edu/doi/abs/10.1080/09540091.2017.1310182?journalCode=ccos20			autonomous robot developers		autonomous robot developers						O1: development and debugging of agents
wexler2020what	2020	Wexler et al.	The What-If Tool: Interactive Probing of Machine Learning Models	https://arxiv-org.libproxy.mit.edu/pdf/1907.04135.pdf		"those with some existing ML experience such as CS students, data scientists, product managers, and ML practitioners."	practitioners					A group of MIT students working on a project for the Foundations of Information Policy course made extensive use of WIT			"O1: debugging; G1: ""medium for model understanding""; T4: explore counterfactuals and how changes to data points affect predictions"
kim2018interpretability	2018	Kim et al.	Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)	https://arxiv.org/abs/1711.11279		model analyst			medical expert						"O1: designing, developing, and debugging models; G2: ensure that ML models reflect our values"
samek2017explainable	2017	Samek et al. 	"Explainable Artificial Intelligence: Understanding, Visualizing, and Interpreting Deep Learning Models"	https://www.itu.int/dms_pub/itu-s/opb/journal/S-JOURNAL-ICTF.VOL1-2018-1-P05-PDF-E.pdf											T2: verification of the system for bias/mistakes; O1: debugging/imporvement; O6: learning game strategy (Go); O2: compliance with legislation
ustun2019recourse	2019	Ustun et al.	Actionable Recourse in Linear Classification	https://arxiv-org.libproxy.mit.edu/pdf/1809.06514.pdf										affected decision subjects / loan applicants	O3: understand how to get the desired outcome; O7: contest the decision if the desired outcome is discriminatory; T4: understand the influence of different factors to see what will change the outcome